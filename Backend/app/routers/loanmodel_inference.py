# -*- coding: utf-8 -*-
"""loanmodel_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0rI2-kR8hDGnYxsFTQ6vyuLzncW-tqY
"""
import os
import pandas as pd
import joblib
import json
import xgboost as xgb
from sklearn.preprocessing import OneHotEncoder

# Go one level up from current file (routers/) to app/
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Construct correct paths
onehot_path = os.path.join(base_dir, 'onehot_encoders_loan.joblib')
xgb_path = os.path.join(base_dir, 'xgb_classifier_loan.joblib')

# Load models
onehot_encoders = joblib.load(onehot_path)
xgb_clf = joblib.load(xgb_path)

# Mapping back from prediction
risk_reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}

def preprocess_input(json_data):
    import pandas as pd

    print("Received JSON data for preprocessing:", json_data)
    input_df = pd.DataFrame([json_data])

    # Drop irrelevant columns
    input_df = input_df.drop(["city", "district", "customer_id"], axis=1, errors='ignore')
    #print("After dropping irrelevant columns:", input_df)

    # Ensure numerical fields are correctly converted
    numerical_fallback = ["loan_amount_requested", "loan_term_months"]
    for col in numerical_fallback:
        if col in input_df.columns:
            input_df[col] = pd.to_numeric(input_df[col], errors='coerce')

    # Dynamically select categorical columns that actually exist
    categorical_cols = [col for col in onehot_encoders.keys() if col in input_df.columns]
    numerical_cols = [col for col in input_df.columns if col not in categorical_cols]
    
    print("Categorical columns to encode:", categorical_cols)
    print("Numerical columns to keep:", numerical_cols)

    # Encode categorical
    processed_df = pd.DataFrame(index=input_df.index)
    for col in categorical_cols:
        encoder = onehot_encoders[col]
        try:
            encoded_feature = encoder.transform(input_df[[col]])
            feature_names = encoder.get_feature_names_out([col])
            encoded_df = pd.DataFrame(encoded_feature, columns=feature_names, index=input_df.index)
            processed_df = pd.concat([processed_df, encoded_df], axis=1)
        except Exception as e:
            print(f"Encoding failed for column '{col}':", e)

    # Append numerical
    numerical_df = input_df[numerical_cols]
    final_input_df = pd.concat([processed_df, numerical_df], axis=1)

    #print("Final preprocessed DataFrame:", final_input_df)
    return final_input_df


def predict_risk(json_data):
    # Preprocess input
    print(json_data)
    processed_input = preprocess_input(json_data)
    print("Processed input DataFrame for prediction:\n", processed_input)

    # Ensure columns match what the model expects
    model_features = xgb_clf.get_booster().feature_names
    missing_cols = set(model_features) - set(processed_input.columns)
    extra_cols = set(processed_input.columns) - set(model_features)

    # Add missing columns as 0
    for col in missing_cols:
        processed_input[col] = 0

    # Drop any unexpected columns
    processed_input = processed_input[model_features]

    # Make prediction
    try:
        pred = xgb_clf.predict(processed_input)[0]
        print("Raw prediction:", pred)
        return risk_reverse_mapping.get(pred, "unknown")
    except Exception as e:
        print("Prediction failed:", e)
        return "error"
